\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wolf-etal-2020-transformers}
\citation{gu2023mamba}
\citation{shi2024wonderfulmatrices}
\citation{lieber2024jamba}
\newlabel{submission}{{1}{1}{}{section.1}{}}
\newlabel{submission@cref}{{[section][1][]1}{[1][1][]1}}
\citation{mamba2}
\citation{lin2021efficient}
\citation{shi2024wonderfulmatrices}
\newlabel{fig:architecture}{{1}{2}{\textbf {Self-Attention before LM-Head}. The state-space and self-attention both use the same positional encoding, and a Transformer block composed of self-attention and feed-forward networks is used before the LM-Head, regardless of how the other parts of the model backbone are combined}{figure.1}{}}
\newlabel{fig:architecture@cref}{{[figure][1][]1}{[1][1][]2}}
\citation{gu2023mamba}
\bibdata{paper}
\bibcite{gu2023mamba}{{1}{2023}{{Gu \& Dao}}{{Gu and Dao}}}
\bibcite{lieber2024jamba}{{2}{2024}{{Lieber et~al.}}{{Lieber, Lenz, Bata, Cohen, Osin, Dalmedigos, Safahi, Meirom, Belinkov, Shalev-Shwartz, et~al.}}}
\bibcite{shi2024wonderfulmatrices}{{3}{2024}{{Shi \& Wu}}{{Shi and Wu}}}
\bibcite{wolf-etal-2020-transformers}{{4}{2020}{{Wolf et~al.}}{{Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush}}}
\bibstyle{icml2025}
\gdef \@abspage@last{4}
