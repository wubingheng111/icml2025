\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wolf-etal-2020-transformers}
\citation{gu2023mamba}
\citation{shi2024wonderfulmatrices}
\citation{lieber2024jamba}
\newlabel{submission}{{1}{1}{}{section.1}{}}
\newlabel{submission@cref}{{[section][1][]1}{[1][1][]1}}
\newlabel{eq:state_space}{{2}{1}{}{equation.2.2}{}}
\newlabel{eq:state_space@cref}{{[equation][2][]2}{[1][1][]1}}
\citation{mamba2}
\citation{kitaev2020reformer}
\citation{lieber2024jamba}
\newlabel{fig:architecture}{{1}{2}{\textbf {Self-Attention before LM-Head}. The state-space and self-attention both use the same positional encoding, and a Transformer block composed of self-attention and feed-forward networks is used before the LM-Head, regardless of how the other parts of the model backbone are combined}{figure.1}{}}
\newlabel{fig:architecture@cref}{{[figure][1][]1}{[1][1][]2}}
\newlabel{eq:self_attention}{{3}{2}{}{equation.2.3}{}}
\newlabel{eq:self_attention@cref}{{[equation][3][]3}{[1][2][]2}}
\citation{gu2023mamba}
\citation{lieber2024jamba}
\citation{bookcrossing_dataset}
\citation{wikipedia_dataset}
\citation{uncorpus_dataset}
\citation{translation2019zh_dataset}
\citation{wikimatri_dataset}
\citation{newscommentary_dataset}
\citation{paracrawl_dataset}
\citation{cluecorpusSmall_dataset}
\citation{li-etal-2022-csl}
\citation{newscrawl_dataset}
\citation{wolf-etal-2020-transformers}
\citation{du2022glm}
\citation{pytorch}
\citation{xu-etal-2020-clue}
\citation{xu-etal-2020-clue}
\newlabel{sec:Self-Attention_before_LM-Head_is_Important}{{4.2}{3}{}{subsection.4.2}{}}
\newlabel{sec:Self-Attention_before_LM-Head_is_Important@cref}{{[subsection][2][4]4.2}{[1][3][]3}}
\citation{bisk2020piqa}
\citation{kovcisky2018narrativeqa}
\citation{yang2018hotpotqa}
\citation{bisk2020piqa}
\citation{kovcisky2018narrativeqa}
\citation{yang2018hotpotqa}
\newlabel{tab:Jamba_vs_Jamba-SABLH_Params}{{1}{4}{\textbf {Parameters of Jamba and Jamba-SABLH for Verification}. In the original Jamba paper, every 8 layers are a hybrid cycle, including 7 state-space modules, 1 self-attention module, and 8 MLP modules. We only modified the last module to a self-attention module, and the rest of the parameters are the same as Jamba. We constructed two models of about 280M, Jamba and Jamba-SABLH. In the hybrid structure, S represents the state-space module, A represents the self-attention module, and M represents the MLP module}{table.1}{}}
\newlabel{tab:Jamba_vs_Jamba-SABLH_Params@cref}{{[table][1][]1}{[1][3][]4}}
\newlabel{tab:Jamba_vs_Jamba-SABLH_Results}{{2}{4}{\textbf {Results of Jamba and Jamba-SABLH for Verification}. The performance gap between Jamba and Jamba-SABLH in pre-training perplexity is not large, but in SFT fine-tuning, the perplexity of Jamba-SABLH is significantly lower than Jamba. We selected the evaluation tasks in CLUEbenchmark~\cite {xu-etal-2020-clue} that are more suitable for the current parameter volume: AFQMC: Semantic similarity, judge whether two sentences have the same meaning or not. TNEWS: Short text classification, from the news section of today's headlines, judge the category of news. IFLYTEK: Long text classification, a long text dataset about software descriptions, judge the application topic of the software description. CMNLI: Language reasoning, judge the logical relationship between two sentences. WSC: Pronoun disambiguation, judge which noun the pronoun in the sentence refers to. CSL: Keyword recognition, taken from the abstract and keywords of the paper, judge whether the keyword is in the abstract. In the validation set inference accuracy of AFQMC, TNEWS, IFLYTEK, WSC, CSL, Jamba-SABLH maintains the lead, slightly behind in CMNLI, and the average accuracy of Jamba-SABLH is $4.25\%$ higher than Jamba}{table.2}{}}
\newlabel{tab:Jamba_vs_Jamba-SABLH_Results@cref}{{[table][2][]2}{[1][3][]4}}
\citation{mamba2}
\citation{grattafiori2024llama3herdmodels}
\citation{mamba2}
\citation{grattafiori2024llama3herdmodels}
\newlabel{tab:associative_recall_params}{{3}{5}{\textbf {Parameters of Jamba and Jamba-SABLH for Associative Recall}. The hybrid cycle of Jamba is equivalent to the hybrid configuration in Table~\ref {tab:Jamba_vs_Jamba-SABLH_Params} three times, and Jamba-SABLH is the same as Jamba except for the last layer of the self-attention module. In addition, we use mixture of experts as the feed-forward network to reduce training costs. We carefully adjusted $d_{ffn}$ to keep the parameter volume at 1B}{table.3}{}}
\newlabel{tab:associative_recall_params@cref}{{[table][3][]3}{[1][4][]5}}
\newlabel{tab:associative_recall_results}{{4}{5}{\textbf {Results of Jamba and Jamba-SABLH for Associative Recall}. We selected associative recall validation tasks including: standard short context task: PIQA~\cite {bisk2020piqa}, natural long context task: NarrativeQA~\cite {kovcisky2018narrativeqa}, synthetic long context task: HotpotQA~\cite {yang2018hotpotqa}. We evaluated the performance of Jamba and Jamba-SABLH on these three tasks. Jamba-SABLH improved by $1.1\%$ on the standard short context task, $20.86\%$ on the natural long context task, $24.15\%$ on the synthetic long context task, and an average improvement of $10.24\%$}{table.4}{}}
\newlabel{tab:associative_recall_results@cref}{{[table][4][]4}{[1][4][]5}}
\newlabel{sec:Associative_Recall}{{4.3}{5}{}{subsection.4.3}{}}
\newlabel{sec:Associative_Recall@cref}{{[subsection][3][4]4.3}{[1][5][]5}}
\bibdata{paper}
\bibcite{bisk2020piqa}{{1}{2020}{{Bisk et~al.}}{{Bisk, Zellers, Gao, Choi, et~al.}}}
\bibcite{translation2019zh_dataset}{{2}{2019}{{Brightmart}}{{}}}
\newlabel{fig:niah}{{2}{6}{\textbf {Needle in a Haystack}. Performance of models with different architectures on the Needle in a Haystack task. We evaluated Mamba2~\cite {mamba2} using state-space alone, Llama3~\cite {grattafiori2024llama3herdmodels} using self-attention alone, Jamba using a hybrid algorithm of state-space and self-attention, and our method Jamba-SABLH on different context lengths. Our method shows the best performance}{figure.2}{}}
\newlabel{fig:niah@cref}{{[figure][2][]2}{[1][5][]6}}
\bibcite{paracrawl_dataset}{{3}{2024}{{Coordination}}{{}}}
\bibcite{mamba2}{{4}{2024}{{Dao \& Gu}}{{Dao and Gu}}}
\bibcite{du2022glm}{{5}{2022}{{Du et~al.}}{{Du, Qian, Liu, Ding, Qiu, Yang, and Tang}}}
\bibcite{newscommentary_dataset}{{6}{2024{a}}{{for Machine~Translation}}{{}}}
\bibcite{newscrawl_dataset}{{7}{2024{b}}{{for Machine~Translation}}{{}}}
\bibcite{wikipedia_dataset}{{8}{2024}{{Foundation}}{{}}}
\bibcite{grattafiori2024llama3herdmodels}{{9}{2024}{{Grattafiori et~al.}}{{Grattafiori, Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Vaughan, Yang, Fan, Goyal, Hartshorn, Yang, Mitra, Sravankumar, Korenev, Hinsvark, Rao, Zhang, Rodriguez, Gregerson, Spataru, Roziere, Biron, Tang, Chern, Caucheteux, Nayak, Bi, Marra, McConnell, Keller, Touret, Wu, Wong, Ferrer, Nikolaidis, Allonsius, Song, Pintz, Livshits, Wyatt, Esiobu, Choudhary, Mahajan, Garcia-Olano, Perino, Hupkes, Lakomkin, AlBadawy, Lobanova, Dinan, Smith, Radenovic, Guzmán, Zhang, Synnaeve, Lee, Anderson, Thattai, Nail, Mialon, Pang, Cucurell, Nguyen, Korevaar, Xu, Touvron, Zarov, Ibarra, Kloumann, Misra, Evtimov, Zhang, Copet, Lee, Geffert, Vranes, Park, Mahadeokar, Shah, van~der Linde, Billock, Hong, Lee, Fu, Chi, Huang, Liu, Wang, Yu, Bitton, Spisak, Park, Rocca, Johnstun, Saxe, Jia, Alwala, Prasad, Upasani, Plawiak, Li, Heafield, Stone, El-Arini, Iyer, Malik, Chiu, Bhalla, Lakhotia, Rantala-Yeary, van~der Maaten, Chen, Tan, Jenkins, Martin, Madaan, Malo, Blecher, Landzaat, de~Oliveira, Muzzi, Pasupuleti, Singh, Paluri, Kardas, Tsimpoukelli, Oldham, Rita, Pavlova, Kambadur, Lewis, Si, Singh, Hassan, Goyal, Torabi, Bashlykov, Bogoychev, Chatterji, Zhang, Duchenne, Çelebi, Alrassy, Zhang, Li, Vasic, Weng, Bhargava, Dubal, Krishnan, Koura, Xu, He, Dong, Srinivasan, Ganapathy, Calderer, Cabral, Stojnic, Raileanu, Maheswari, Girdhar, Patel, Sauvestre, Polidoro, Sumbaly, Taylor, Silva, Hou, Wang, Hosseini, Chennabasappa, Singh, Bell, Kim, Edunov, Nie, Narang, Raparthy, Shen, Wan, Bhosale, Zhang, Vandenhende, Batra, Whitman, Sootla, Collot, Gururangan, Borodinsky, Herman, Fowler, Sheasha, Georgiou, Scialom, Speckbacher, Mihaylov, Xiao, Karn, Goswami, Gupta, Ramanathan, Kerkez, Gonguet, Do, Vogeti, Albiero, Petrovic, Chu, Xiong, Fu, Meers, Martinet, Wang, Wang, Tan, Xia, Xie, Jia, Wang, Goldschlag, Gaur, Babaei, Wen, Song, Zhang, Li, Mao, Coudert, Yan, Chen, Papakipos, Singh, Srivastava, Jain, Kelsey, Shajnfeld, Gangidi, Victoria, Goldstand, Menon, Sharma, Boesenberg, Baevski, Feinstein, Kallet, Sangani, Teo, Yunus, Lupu, Alvarado, Caples, Gu, Ho, Poulton, Ryan, Ramchandani, Dong, Franco, Goyal, Saraf, Chowdhury, Gabriel, Bharambe, Eisenman, Yazdan, James, Maurer, Leonhardi, Huang, Loyd, Paola, Paranjape, Liu, Wu, Ni, Hancock, Wasti, Spence, Stojkovic, Gamido, Montalvo, Parker, Burton, Mejia, Liu, Wang, Kim, Zhou, Hu, Chu, Cai, Tindal, Feichtenhofer, Gao, Civin, Beaty, Kreymer, Li, Adkins, Xu, Testuggine, David, Parikh, Liskovich, Foss, Wang, Le, Holland, Dowling, Jamil, Montgomery, Presani, Hahn, Wood, Le, Brinkman, Arcaute, Dunbar, Smothers, Sun, Kreuk, Tian, Kokkinos, Ozgenel, Caggioni, Kanayet, Seide, Florez, Schwarz, Badeer, Swee, Halpern, Herman, Sizov, Guangyi, Zhang, Lakshminarayanan, Inan, Shojanazeri, Zou, Wang, Zha, Habeeb, Rudolph, Suk, Aspegren, Goldman, Zhan, Damlaj, Molybog, Tufanov, Leontiadis, Veliche, Gat, Weissman, Geboski, Kohli, Lam, Asher, Gaya, Marcus, Tang, Chan, Zhen, Reizenstein, Teboul, Zhong, Jin, Yang, Cummings, Carvill, Shepard, McPhie, Torres, Ginsburg, Wang, Wu, U, Saxena, Khandelwal, Zand, Matosich, Veeraraghavan, Michelena, Li, Jagadeesh, Huang, Chawla, Huang, Chen, Garg, A, Silva, Bell, Zhang, Guo, Yu, Moshkovich, Wehrstedt, Khabsa, and et~al.}}}
\bibcite{gu2023mamba}{{10}{2023}{{Gu \& Dao}}{{Gu and Dao}}}
\bibcite{kitaev2020reformer}{{11}{2020}{{Kitaev et~al.}}{{Kitaev, Kaiser, and Levskaya}}}
\bibcite{kovcisky2018narrativeqa}{{12}{2018}{{Kovcisky et~al.}}{{Kovcisky, McCann, Bradbury, Xiong, and Socher}}}
\bibcite{li-etal-2022-csl}{{13}{2022}{{Li et~al.}}{{Li, Zhang, Zhao, Shen, Liu, Mao, and Zhang}}}
\bibcite{lieber2024jamba}{{14}{2024}{{Lieber et~al.}}{{Lieber, Lenz, Bata, Cohen, Osin, Dalmedigos, Safahi, Meirom, Belinkov, Shalev-Shwartz, et~al.}}}
\bibcite{uncorpus_dataset}{{15}{2024}{{Nations}}{{}}}
\bibcite{pytorch}{{16}{2022}{{NVIDIA}}{{}}}
\bibcite{shi2024wonderfulmatrices}{{17}{2024}{{Shi \& Wu}}{{Shi and Wu}}}
\bibcite{cluecorpusSmall_dataset}{{18}{2020}{{Team}}{{}}}
\bibcite{wikimatri_dataset}{{19}{2024}{{Unknown}}{{}}}
\bibcite{wolf-etal-2020-transformers}{{20}{2020}{{Wolf et~al.}}{{Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush}}}
\bibcite{xu-etal-2020-clue}{{21}{2020}{{Xu et~al.}}{{Xu, Hu, Zhang, Li, Cao, Li, Xu, Sun, Yu, Yu, Tian, Dong, Liu, Shi, Cui, Li, Zeng, Wang, Xie, Li, Patterson, Tian, Zhang, Zhou, Liu, Zhao, Zhao, Yue, Zhang, Yang, Richardson, and Lan}}}
\bibcite{yang2018hotpotqa}{{22}{2018}{{Yang et~al.}}{{Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and Manning}}}
\bibcite{bookcrossing_dataset}{{23}{2004}{{Ziegler}}{{}}}
\bibstyle{icml2025}
\gdef \@abspage@last{8}
